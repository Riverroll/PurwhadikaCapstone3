{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context:\n",
    "In the telecommunications industry, customer churn is a significant concern as it directly impacts revenue and profitability. The dataset represents customer profiles of those who have left a telco company. Understanding the factors that contribute to customer churn can help the company develop strategies to retain existing customers and improve overall customer satisfaction.\n",
    "\n",
    "### Problem Statement:\n",
    "The telco company wants to identify the customers who are likely to churn or leave the company. By understanding the characteristics and behavior patterns of these customers, the company can take proactive measures to prevent churn and implement targeted retention strategies.\n",
    "\n",
    "### Goals:\n",
    "\n",
    "- Develop a predictive model that can accurately identify customers who are likely to churn based on their profile information and service usage patterns.\n",
    "- Gain insights into the key factors or variables that are closely associated with customer churn, such as tenure, services subscribed, contract type, monthly charges, and customer demographics.\n",
    "- Use the insights and the predictive model to develop effective customer retention strategies and personalized offers or incentives to reduce churn.\n",
    "\n",
    "### Analytic Approach:\n",
    "\n",
    "- Perform exploratory data analysis (EDA) to understand the dataset, identify any missing or inconsistent data, and gain initial insights into potential relationships between features and customer churn.\n",
    "- Preprocess the data by handling missing values, encoding categorical variables, and scaling numerical features, if necessary.\n",
    "- Split the data into training and testing sets.\n",
    "- Build and evaluate various machine learning classification models (e.g., logistic regression, decision trees, random forests, gradient boosting) to predict customer churn.\n",
    "- Optimize the best-performing model(s) through techniques like hyperparameter tuning and ensemble methods.\n",
    "- Interpret the model results to identify the most important features influencing customer churn.\n",
    "- Validate the model's performance on the test set using appropriate evaluation metrics.\n",
    "\n",
    "### Metric Evaluation:\n",
    "The primary evaluation metric for this classification problem will be the Area Under the Receiver Operating Characteristic (ROC-AUC) curve. The ROC-AUC provides a comprehensive measure of the model's ability to distinguish between churners and non-churners across different classification thresholds.\n",
    "\n",
    "Additionally, we can consider other relevant metrics, such as:\n",
    "- Precision: The proportion of correctly identified churners among all predicted churners (to minimize false positives).\n",
    "- Recall: The proportion of correctly identified churners among all actual churners (to minimize false negatives).\n",
    "- F1-score: The harmonic mean of precision and recall, providing a balanced measure of model performance.\n",
    "The choice of metric(s) will depend on the business priorities and the relative importance of minimizing false positives (incorrectly identifying non-churners as churners) or false negatives (missing potential churners).\n",
    "\n",
    "By analyzing the model's performance using these metrics, the telco company can strike a balance between proactive customer retention efforts and efficient resource allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "df = pd.read_csv(\"telcocucu.csv\")\n",
    "\n",
    "# Preprocess data\n",
    "categorical_cols = ['Dependents', 'OnlineSecurity', 'OnlineBackup', 'InternetService', 'DeviceProtection', 'TechSupport', 'Contract', 'PaperlessBilling']\n",
    "numerical_cols = ['tenure', 'MonthlyCharges']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the first line loads a CSV file into a Pandas DataFrame, and the next two lines identify which columns in the DataFrame contain categorical data and which columns contain numerical data. This information is typically needed for preprocessing the data before training a machine learning model, as categorical and numerical data often need to be handled differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['Churn'])\n",
    "\n",
    "# Split data into features and target\n",
    "X = df.drop('Churn', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a LabelEncoder object to encode categorical variables.\n",
    "2. Encode the 'Churn' column (which is likely a categorical variable indicating whether a customer has churned or not) into numerical values, and store the encoded values in the variable y.\n",
    "3. Create a new DataFrame X that contains all the features (predictor variables) except for the 'Churn' column.\n",
    "\n",
    "The next step would typically be to split the data (X and y) into training and testing sets, and then train a machine learning model using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for string values in X\n",
    "string_cols = [col for col in X.columns if X[col].dtype == 'object']\n",
    "\n",
    "# Encode string values in X\n",
    "if string_cols:\n",
    "    le = LabelEncoder()\n",
    "    for col in string_cols:\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this line of code creates a Pipeline object called categorical_transformer that contains a single step: encoding categorical features using the OneHotEncoder with the handle_unknown='ignore' parameter.\n",
    "This categorical_transformer object can then be used in conjunction with other transformers and estimators to preprocess the categorical features and train a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess numerical features (if needed)\n",
    "numerical_transformer = 'passthrough'\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        ('num', numerical_transformer, numerical_cols)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these lines of code create a ColumnTransformer object called preprocessor that can apply different transformations to the categorical and numerical columns of the dataset. The categorical columns will be encoded using the OneHotEncoder, while the numerical columns will be left untransformed (due to the 'passthrough' keyword).\n",
    "The preprocessor object can then be used as part of a Pipeline along with a machine learning model to preprocess the data and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply the preprocessing to the feature matrix\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Oversample the minority class\n",
    "smote = SMOTE()\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectFromModel(estimator=RandomForestClassifier(), threshold='median')\n",
    "X_train_selected = selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Prepare pipeline with Random Forest classifier\n",
    "classifier_pipeline = Pipeline(steps=[\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [3, 5, 7],\n",
    "    'classifier__n_estimators': [100, 200, 300]\n",
    "}\n",
    "grid_search = GridSearchCV(classifier_pipeline, param_grid, cv=5, scoring='roc_auc')\n",
    "grid_search.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Get the best estimator\n",
    "best_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Train the model\n",
    "best_classifier.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_classifier.predict(X_test_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " these lines of code split the data into training and testing sets, create a Pipeline that preprocesses the data and trains a LogisticRegression model, train the model using the training data, and then use the trained model to make predictions on the test data.\n",
    "Note that you can try different classifiers (e.g., DecisionTreeClassifier, RandomForestClassifier, XGBClassifier, etc.) by replacing LogisticRegression() in the classifier_pipeline with the desired classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.75\n",
      "Precision: 0.60\n",
      "Recall: 0.70\n",
      "F1-score: 0.65\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"ROC-AUC: {roc_auc:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These evaluation metrics provide insights into the performance of the trained model. The ROC-AUC score measures the overall ability of the model to distinguish between classes, while precision, recall, and F1 score give more specific information about the model's performance in terms of correctly identifying positive and negative instances.\n",
    "By printing these scores, you can assess the model's performance and potentially compare it to other models or baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
